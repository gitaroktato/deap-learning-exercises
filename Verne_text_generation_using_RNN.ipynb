{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gitaroktato/deep-learning-exercises/blob/verne-rnn-generator/%20Verne_text_generation_using_RNN..ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smCVdpCXifWC"
   },
   "source": [
    "# Text generation in the style of Jules Verne using RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H766SuuViZ7q",
    "outputId": "e14d6b9c-fd8a-4496-9217-8f6dbdb4d0cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 16:21:57.355303: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-02 16:21:57.363545: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-02 16:21:57.373544: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-02 16:21:57.376318: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-02 16:21:57.385027: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-02 16:21:57.965427: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.20\n",
      "Tensorflow version: 2.17.0\n",
      "Keras version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import platform\n",
    "import time\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "print('Python version:', platform.python_version())\n",
    "print('Tensorflow version:', tf.__version__)\n",
    "print('Keras version:', tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cen2gAO0i2uA"
   },
   "source": [
    "## Loading the dataset\n",
    "Load the dataset and read the data. Take a look in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KGnb8Nk3i5xY",
    "outputId": "19162988-65dd-4d0b-c341-9cd538639835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 662630 characters\n"
     ]
    }
   ],
   "source": [
    "path_to_file = './docs/utazas_a_holdra.txt'\n",
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-K_dp5z6jYwx",
    "outputId": "7cc3da4a-1ba4-4c84-d7e6-8ba53087a57b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULES VERNE\n",
      "UTAZÁS A HOLDBA\n",
      "______\n",
      "UTAZÁS A HOLD KÖRÜL\n",
      "KÉT REGÉNY\n",
      "FORDÍTOTTA: KILÉNYI MÁRIA\n",
      "2\n",
      "TARTALOM\n",
      "UTAZÁS A HOLDBA\n",
      "1. A GUN CLUB\n",
      "2. BARBICANE ELNÖK BEJELENTÉSE\n",
      "3. A BEJELENTÉS HATÁSA\n",
      "4. A CAMBRIDGE-I CSILLAGVIZSGÁLÓ VÁLASZA\n",
      "5. A HOLD REGÉNYE\n",
      "6. A\n"
     ]
    }
   ],
   "source": [
    "# First few characters of the text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2O5JXa8_jjZ1",
    "outputId": "0781cecc-b77b-469f-c0f7-9399bb451fc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118 unique characters\n",
      "vocab: ['\\n', ' ', '!', '%', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '°', 'Á', 'É', 'Í', 'Ó', 'Ö', 'Ú', 'Ü', 'á', 'è', 'é', 'í', 'ó', 'ô', 'ö', 'ú', 'ü', 'Ő', 'ő', 'Ű', 'ű', 'π', '’', '”', '„', '−', '\\uf8eb', '\\uf8ec', '\\uf8ed', '\\uf8ee', '\\uf8ef', '\\uf8f0', '\\uf8f6', '\\uf8f7', '\\uf8f8', '\\uf8f9', '\\uf8fa', '\\uf8fb']\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file. Note, that it contains Hungarian character set.\n",
    "vocab = sorted(set(text))\n",
    "\n",
    "print('{} unique characters'.format(len(vocab)))\n",
    "print('vocab:', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F42fiPq2jt0_"
   },
   "source": [
    "# Process the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IKlLnwNJkLjJ",
    "outputId": "73f7e063-4214-49c0-804e-dcb53e7f067b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727878934.071622    2007 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1727878934.089278    2007 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1727878934.089318    2007 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1727878934.090805    2007 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1727878934.090842    2007 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1727878934.090857    2007 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1727878934.159872    2007 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1727878934.159925    2007 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-02 16:22:14.159933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1727878934.159964    2007 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-02 16:22:14.159977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5997 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   1,\n",
      "  ' ' :   2,\n",
      "  '!' :   3,\n",
      "  '%' :   4,\n",
      "  \"'\" :   5,\n",
      "  '(' :   6,\n",
      "  ')' :   7,\n",
      "  '*' :   8,\n",
      "  '+' :   9,\n",
      "  ',' :  10,\n",
      "  '-' :  11,\n",
      "  '.' :  12,\n",
      "  '/' :  13,\n",
      "  '0' :  14,\n",
      "  '1' :  15,\n",
      "  '2' :  16,\n",
      "  '3' :  17,\n",
      "  '4' :  18,\n",
      "  '5' :  19,\n",
      "  '6' :  20,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)\n",
    "\n",
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "\n",
    "print('{')\n",
    "for char, _ in zip(vocab, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), ids_from_chars(char)))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XNLgvvcWs4RF",
    "outputId": "5bc9cbc4-4b27-4549-f5a7-3bffeb29c7b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_as_int length: 662630\n",
      "'JULES VERNE\\nUTA' --> array([37, 48, 39, 32, 46,  2, 49, 32, 45, 41, 32,  1, 48, 47, 28])\n"
     ]
    }
   ],
   "source": [
    "# Convert chars in text to indices.\n",
    "text_as_int = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "\n",
    "print('text_as_int length: {}'.format(len(text_as_int)))\n",
    "print('{} --> {}'.format(repr(text[:15]), repr(text_as_int[:15].numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQCgjcUhvtOE"
   },
   "source": [
    "## Create training examples and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KuopxsvYvwWa",
    "outputId": "67f147e0-5404-4a75-f202-3fd6d3820b02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples_per_epoch: 6560\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters.\n",
    "sequence_length = 100\n",
    "examples_per_epoch = len(text) // (sequence_length + 1)\n",
    "\n",
    "print('examples_per_epoch:', examples_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zmmgnTXnv2Cl",
    "outputId": "3ef6012e-2bb2-44dc-af76-37baca83e154"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J\n",
      "U\n",
      "L\n",
      "E\n",
      "S\n",
      " \n",
      "V\n",
      "E\n",
      "R\n",
      "N\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 16:22:23.124932: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Create training dataset.\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for id in ids_dataset.take(10):\n",
    "    print(chars_from_ids(id).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gRTHHV3KwThJ",
    "outputId": "7964f66b-e81c-4673-b73d-d3f61434cf77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences count: 6560\n",
      "\n",
      "JULES VERNE\n",
      "UTAZÁS A HOLDBA\n",
      "______\n",
      "UTAZÁS A HOLD KÖRÜL\n",
      "KÉT REGÉNY\n",
      "FORDÍTOTTA: KILÉNYI MÁRIA\n",
      "2\n",
      "TARTALO\n",
      "M\n",
      "UTAZÁS A HOLDBA\n",
      "1. A GUN CLUB\n",
      "2. BARBICANE ELNÖK BEJELENTÉSE\n",
      "3. A BEJELENTÉS HATÁSA\n",
      "4. A CAMBRIDGE-\n",
      "I CSILLAGVIZSGÁLÓ VÁLASZA\n",
      "5. A HOLD REGÉNYE\n",
      "6. AMIT MINDEN AMERIKAINAK TUDNIA KELL, ÉS AMIT EGYETLEN \n",
      "AMERIKAINAK SEM\n",
      "SZABAD HINNIE TÖBBÉ\n",
      "7. AZ ÁGYÚGOLYÓ HIMNUSZA\n",
      "8. AZ ÁGYÚ TÖRTÉNETE\n",
      "9. A LŐPORKÉRDÉS\n",
      "10\n",
      ". HUSZONÖTMILLIÓ BARÁT ÉS EGYETLENEGY ELLENSÉG\n",
      "11. FLORIDA ÉS TEXAS\n",
      "12. URBI ET ORBI\n",
      "13. STONE’S HILL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 16:22:25.382971: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
    "\n",
    "# Generate batched sequences out of the char_dataset.\n",
    "sequences = ids_dataset.batch(sequence_length + 1, drop_remainder=True)\n",
    "\n",
    "# Sequences size is the same as examples_per_epoch.\n",
    "print('Sequences count: {}'.format(len(list(sequences.as_numpy_iterator()))));\n",
    "print()\n",
    "\n",
    "# Sequences examples.\n",
    "for item in sequences.take(5):\n",
    "    print(text_from_ids(item).numpy().decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gSA60YaJz4Wu"
   },
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ChW0Y7i4z8Ob",
    "outputId": "9d7356c8-54bb-472e-edc6-1564d45655db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TVWfXsFe0A-_",
    "outputId": "173e3095-5d86-479c-ab5c-4c94cc8b7fef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 6560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 16:22:33.198635: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "# Dataset size is the same as examples_per_epoch.\n",
    "# But each element of a sequence is now has length of `sequence_length`\n",
    "# and not `sequence_length + 1`.\n",
    "print('dataset size: {}'.format(len(list(dataset.as_numpy_iterator()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24EgwAfo0L9R",
    "outputId": "3e63f541-c175-49e1-e86e-788b7b20c6e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence size: 100\n",
      "Target sequence size: 100\n",
      "\n",
      "Input: JULES VERNE\n",
      "UTAZÁS A HOLDBA\n",
      "______\n",
      "UTAZÁS A HOLD KÖRÜL\n",
      "KÉT REGÉNY\n",
      "FORDÍTOTTA: KILÉNYI MÁRIA\n",
      "2\n",
      "TARTAL\n",
      "Target: ULES VERNE\n",
      "UTAZÁS A HOLDBA\n",
      "______\n",
      "UTAZÁS A HOLD KÖRÜL\n",
      "KÉT REGÉNY\n",
      "FORDÍTOTTA: KILÉNYI MÁRIA\n",
      "2\n",
      "TARTALO\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print('Input sequence size:', repr(len(input_example.numpy())))\n",
    "    print('Target sequence size:', repr(len(target_example.numpy())))\n",
    "    print()\n",
    "    print('Input:', text_from_ids(input_example).numpy().decode('UTF-8'))\n",
    "    print('Target:', text_from_ids(target_example).numpy().decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIbRGsaM0Zs5",
    "outputId": "c08f7b65-b947-41f2-8b62-922424189beb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  0\n",
      "  input: 37 (J)\n",
      "  expected output: 48 (U)\n",
      "Step  1\n",
      "  input: 48 (U)\n",
      "  expected output: 39 (L)\n",
      "Step  2\n",
      "  input: 39 (L)\n",
      "  expected output: 32 (E)\n",
      "Step  3\n",
      "  input: 32 (E)\n",
      "  expected output: 46 (S)\n",
      "Step  4\n",
      "  input: 46 (S)\n",
      "  expected output: 2 ( )\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print('Step {:2d}'.format(i))\n",
    "    print('  input: {} ({:s})'.format(input_idx, chars_from_ids(input_idx).numpy().decode('UTF-8')))\n",
    "    print('  expected output: {} ({:s})'.format(target_idx, chars_from_ids(target_idx).numpy().decode('UTF-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0svwTVeQMSvO"
   },
   "source": [
    "## Create training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pzfN6jY6MW7J",
    "outputId": "61c019ad-af31-4b9c-8be3-7a1f765bdcd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CW3ibDNoMmK_",
    "outputId": "38daf05c-a1ec-4fa1-c5ad-27d66db04ff1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched dataset size: 102\n"
     ]
    }
   ],
   "source": [
    "print('Batched dataset size: {}'.format(len(list(dataset.as_numpy_iterator()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VdeCBH2UMpiV",
    "outputId": "f0a924df-d22d-4d2a-dad9-c22930734926"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st batch: input_text: tf.Tensor(\n",
      "[[60 59 73 ... 59 80 80]\n",
      " [57 73 91 ... 66 67 55]\n",
      " [63 74 59 ... 61 59 73]\n",
      " ...\n",
      " [ 2 76 63 ... 89 74 69]\n",
      " [66 59 73 ... 59 66 74]\n",
      " [66 66 93 ... 59 61 92]], shape=(64, 100), dtype=int64)\n",
      "\n",
      "1st batch: target_text: tf.Tensor(\n",
      "[[59 73 80 ... 80 80 59]\n",
      " [73 91 74 ... 67 55 80]\n",
      " [74 59 72 ... 59 73 59]\n",
      " ...\n",
      " [76 63 73 ... 74 69 67]\n",
      " [59 73 80 ... 66 74 59]\n",
      " [66 93  1 ... 61 92 74]], shape=(64, 100), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for input_text, target_text in dataset.take(1):\n",
    "    print('1st batch: input_text:', input_text)\n",
    "    print()\n",
    "    print('1st batch: target_text:', target_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_wmq4O2M_NV"
   },
   "source": [
    "## Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8rEHAg9LNB1c",
    "outputId": "a1ea7019-13b7-408a-f1a6-45bd825db65b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gitaroktato/Projects/deep-learning-exercises/.venv/lib/python3.9/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727878960.345240    2053 service.cc:146] XLA service 0x7fe300003f80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1727878960.345281    2053 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 2070, Compute Capability 7.5\n",
      "2024-10-02 16:22:40.397325: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305ms/step\n",
      "tmp_input_array shape: (2, 8)\n",
      "tmp_input_array:\n",
      "[[5 5 7 0 8 0 8 5]\n",
      " [7 1 9 6 3 6 9 1]]\n",
      "\n",
      "tmp_output_array shape: (2, 8, 5)\n",
      "tmp_output_array:\n",
      "[[[ 0.00185864 -0.02100599 -0.01260793 -0.02232742 -0.03880433]\n",
      "  [ 0.00185864 -0.02100599 -0.01260793 -0.02232742 -0.03880433]\n",
      "  [-0.00946404  0.03090694  0.03300622 -0.0216624   0.01370105]\n",
      "  [ 0.02836508 -0.033585    0.04161905  0.03932513 -0.03019981]\n",
      "  [ 0.03100076  0.0426369   0.025365    0.02066356 -0.03179627]\n",
      "  [ 0.02836508 -0.033585    0.04161905  0.03932513 -0.03019981]\n",
      "  [ 0.03100076  0.0426369   0.025365    0.02066356 -0.03179627]\n",
      "  [ 0.00185864 -0.02100599 -0.01260793 -0.02232742 -0.03880433]]\n",
      "\n",
      " [[-0.00946404  0.03090694  0.03300622 -0.0216624   0.01370105]\n",
      "  [ 0.03315396  0.01271433 -0.03646644 -0.00328575 -0.01731677]\n",
      "  [ 0.02633375  0.0260104  -0.02483853 -0.04419617 -0.00911171]\n",
      "  [ 0.01703577  0.03164811 -0.03299233  0.02200061  0.00288995]\n",
      "  [-0.03768346 -0.04272367 -0.03252758 -0.00411167  0.0351348 ]\n",
      "  [ 0.01703577  0.03164811 -0.03299233  0.02200061  0.00288995]\n",
      "  [ 0.02633375  0.0260104  -0.02483853 -0.04419617 -0.00911171]\n",
      "  [ 0.03315396  0.01271433 -0.03646644 -0.00328575 -0.01731677]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1727878960.574727    2053 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "# Let's do a quick detour and see how Embeding layer works.\n",
    "# It takes several char indices sequences (batch) as an input.\n",
    "# It encodes every character of every sequence to a vector of tmp_embeding_size length.\n",
    "tmp_vocab_size = 10\n",
    "tmp_embeding_size = 5\n",
    "tmp_input_length = 8\n",
    "tmp_batch_size = 2\n",
    "\n",
    "tmp_model = tf.keras.models.Sequential()\n",
    "tmp_model.add(tf.keras.layers.Embedding(\n",
    "  input_dim=tmp_vocab_size,\n",
    "  output_dim=tmp_embeding_size,\n",
    "  input_length=tmp_input_length\n",
    "))\n",
    "# The model will take as input an integer matrix of size (batch, input_length).\n",
    "# The largest integer (i.e. word index) in the input should be no larger than 9 (tmp_vocab_size).\n",
    "# Now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "tmp_input_array = np.random.randint(\n",
    "  low=0,\n",
    "  high=tmp_vocab_size,\n",
    "  size=(tmp_batch_size, tmp_input_length)\n",
    ")\n",
    "tmp_model.compile('rmsprop', 'mse')\n",
    "tmp_output_array = tmp_model.predict(tmp_input_array)\n",
    "\n",
    "print('tmp_input_array shape:', tmp_input_array.shape)\n",
    "print('tmp_input_array:')\n",
    "print(tmp_input_array)\n",
    "print()\n",
    "print('tmp_output_array shape:', tmp_output_array.shape)\n",
    "print('tmp_output_array:')\n",
    "print(tmp_output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4DjgSuB7NRkq"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars.\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension.\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units.\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1YWPEdwvNV41"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Embedding(\n",
    "      input_dim=vocab_size,\n",
    "      output_dim=embedding_dim\n",
    "    ))\n",
    "\n",
    "    model.add(tf.keras.layers.GRU(\n",
    "      units=rnn_units,\n",
    "      return_sequences=True,\n",
    "      stateful=True,\n",
    "      recurrent_initializer=tf.keras.initializers.GlorotNormal()\n",
    "    ))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(vocab_size))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "umanu3FvNeU3"
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4hG7uBGO8f9"
   },
   "source": [
    "## Try the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S34880x-Odjb",
    "outputId": "5a9ed68e-bca2-4d65-9723-1dc7a04f73a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100)\n",
      "(64, 100, 119) # (batch_size, sequence_length, vocab_size)\n",
      "(64, 100)\n",
      "(64, 100, 119) # (batch_size, sequence_length, vocab_size)\n",
      "(64, 100)\n",
      "(64, 100, 119) # (batch_size, sequence_length, vocab_size)\n",
      "(64, 100)\n",
      "(64, 100, 119) # (batch_size, sequence_length, vocab_size)\n",
      "(64, 100)\n",
      "(64, 100, 119) # (batch_size, sequence_length, vocab_size)\n",
      "(64, 100)\n",
      "(64, 100, 119) # (batch_size, sequence_length, vocab_size)\n",
      "(64, 100)\n",
      "(64, 100, 119) # (batch_size, sequence_length, vocab_size)\n",
      "(64, 100)\n",
      "(64, 100, 119) # (batch_size, sequence_length, vocab_size)\n",
      "(64, 100)\n",
      "(64, 100, 119) # (batch_size, sequence_length, vocab_size)\n",
      "(64, 100)\n",
      "(64, 100, 119) # (batch_size, sequence_length, vocab_size)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 16:22:50.808795: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(10):\n",
    "    print(input_example_batch.shape)\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "tC_xiJEtOo7D",
    "outputId": "a6b7c38f-3a1d-49f1-bcf8-f9cc44fe4135"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,938,304</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">119</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">121,975</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │        \u001b[38;5;34m30,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m)        │     \u001b[38;5;34m3,938,304\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m119\u001b[0m)         │       \u001b[38;5;34m121,975\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,090,743</span> (15.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,090,743\u001b[0m (15.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,090,743</span> (15.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,090,743\u001b[0m (15.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wQMG94pYO_5U",
    "outputId": "f8d03b07-c5aa-46cb-e1e4-5fbe77740a2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the 1st letter of the batch 1st sequense:\n",
      "tf.Tensor(\n",
      "[-1.81233871e-03  2.31324565e-02 -1.37233511e-02  1.03080403e-02\n",
      "  1.15984511e-02 -2.59139412e-03  1.26301674e-02 -1.04524344e-02\n",
      " -7.53531558e-03  1.26383239e-02  1.64030287e-02 -9.93223768e-03\n",
      "  9.58671153e-04 -8.63767415e-03  7.57475616e-03  5.37779275e-03\n",
      " -1.66282412e-02 -6.64332928e-03  1.74230430e-02  5.11807762e-03\n",
      "  6.18073205e-03 -8.16850085e-03  2.58266646e-02 -8.54787324e-03\n",
      "  5.87960007e-03  6.12776494e-03  8.73023830e-03  1.17697576e-02\n",
      "  2.04232987e-02 -4.05841693e-03 -2.16349796e-03 -5.95980091e-03\n",
      " -8.68052617e-03 -1.05409522e-03 -2.64295237e-03  2.11315323e-02\n",
      "  4.58633713e-03  4.06197598e-03  8.13912507e-03  7.68713886e-04\n",
      "  1.82122719e-02  5.45066921e-03 -2.91244779e-03  6.75075175e-03\n",
      "  7.70464167e-03  1.25372726e-02 -1.07668969e-03  5.68728568e-03\n",
      " -3.44826723e-04 -2.48803594e-03  1.01106362e-02  6.11858908e-04\n",
      "  3.53057659e-03  7.94468971e-04  1.51895173e-02  1.14990762e-02\n",
      "  1.32699627e-02  1.21411132e-02  2.13541910e-02  7.21867289e-03\n",
      "  1.13122305e-02  4.28056996e-03  1.79503448e-02 -1.49640883e-03\n",
      " -3.03764711e-03  9.68100782e-03 -6.12792419e-03  3.68634961e-03\n",
      " -6.66994462e-03  8.83042754e-04 -1.33856880e-02  1.17687546e-02\n",
      "  7.44513283e-03  1.88870020e-02 -2.95219041e-04 -4.88027139e-03\n",
      " -1.23420512e-04  4.88213869e-03 -4.93162498e-03  6.54725637e-03\n",
      " -8.84503499e-03 -2.53749755e-03  5.00261132e-03 -1.55036878e-02\n",
      "  1.22535555e-02 -2.13047080e-02  1.75034744e-03  2.16146223e-02\n",
      " -2.42723268e-03  2.24552071e-03  5.48316748e-04  1.54916253e-02\n",
      " -3.64691438e-03  4.80995281e-03 -2.27289507e-03  6.21070240e-06\n",
      "  2.04509543e-03  3.79162934e-03  1.17547875e-02 -1.83404458e-03\n",
      " -1.91653557e-02 -9.24672838e-03  1.37212104e-03 -3.68416891e-03\n",
      "  2.33925302e-02 -1.76290330e-03 -7.26219825e-03  3.47471843e-03\n",
      " -1.60612985e-02 -8.66317563e-03  3.27324029e-03  5.28489472e-03\n",
      "  1.12594571e-02  7.34162331e-03  5.96219162e-03  8.91257019e-04\n",
      " -4.43460158e-04  1.80950726e-03 -1.66521582e-03], shape=(119,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('Prediction for the 1st letter of the batch 1st sequense:')\n",
    "print(example_batch_predictions[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9vtg8JMPNiw"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jm_E3PmzPOzh",
    "outputId": "e880f6e2-3e52-4545-8d15-53f9f8b613f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 119)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.7783422\n"
     ]
    }
   ],
   "source": [
    "# An objective function.\n",
    "# The function is any callable with the signature scalar_loss = fn(y_true, y_pred).\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(\n",
    "      y_true=labels,\n",
    "      y_pred=logits,\n",
    "      from_logits=True\n",
    "    )\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "N9fTz4hAPS7O"
   },
   "outputs": [],
   "source": [
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    optimizer=adam_optimizer,\n",
    "    loss=loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3CgV5q0PbVw"
   },
   "source": [
    "## Configure checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Rp-r4sTzPd1N"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJOhfoErPi6k",
    "outputId": "b76b8d75-3934-4e21-acbb-9d2d87e33c4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - loss: 3.6209\n",
      "Epoch 2/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 2.3490\n",
      "Epoch 3/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - loss: 2.0536\n",
      "Epoch 4/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 67ms/step - loss: 1.8303\n",
      "Epoch 5/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 1.6691\n",
      "Epoch 6/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 1.5565\n",
      "Epoch 7/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 1.4740\n",
      "Epoch 8/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 1.4001\n",
      "Epoch 9/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 1.3447\n",
      "Epoch 10/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 1.2919\n",
      "Epoch 11/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 68ms/step - loss: 1.2439\n",
      "Epoch 12/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 1.1967\n",
      "Epoch 13/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 1.1480\n",
      "Epoch 14/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 1.1074\n",
      "Epoch 15/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 1.0644\n",
      "Epoch 16/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 1.0182\n",
      "Epoch 17/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 0.9757\n",
      "Epoch 18/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - loss: 0.9311\n",
      "Epoch 19/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 38ms/step - loss: 0.8892\n",
      "Epoch 20/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 0.8423\n",
      "Epoch 21/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 0.8038\n",
      "Epoch 22/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 0.7637\n",
      "Epoch 23/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 0.7247\n",
      "Epoch 24/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 0.6883\n",
      "Epoch 25/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 0.6542\n",
      "Epoch 26/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 67ms/step - loss: 0.6243\n",
      "Epoch 27/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 0.5955\n",
      "Epoch 28/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 0.5722\n",
      "Epoch 29/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 0.5500\n",
      "Epoch 30/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 0.5274\n",
      "Epoch 31/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 0.5124\n",
      "Epoch 32/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 0.4944\n",
      "Epoch 33/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 67ms/step - loss: 0.4843\n",
      "Epoch 34/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 0.4733\n",
      "Epoch 35/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 0.4601\n",
      "Epoch 36/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 0.4508\n",
      "Epoch 37/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 0.4448\n",
      "Epoch 38/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 0.4381\n",
      "Epoch 39/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 0.4321\n",
      "Epoch 40/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - loss: 0.4267\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=40\n",
    "\n",
    "import keras\n",
    "\n",
    "keras.config.disable_traceback_filtering()\n",
    "# keras.backend.set_image_data_format(\"channels_last\")\n",
    "\n",
    "history = model.fit(\n",
    "  x=dataset,\n",
    "  epochs=EPOCHS,\n",
    "  callbacks=[\n",
    "    checkpoint_callback\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_generate\n",
    "# - number of characters to generate.\n",
    "#\n",
    "# temperature\n",
    "# - Low temperatures results in more predictable text.\n",
    "# - Higher temperatures results in more surprising text.\n",
    "# - Experiment to find the best setting.\n",
    "def generate_text(model, start_string, num_generate = 1000, temperature=1.0):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing).\n",
    "    input_indices = [ids_from_chars(s) for s in start_string]\n",
    "    input_indices = tf.expand_dims(input_indices, 0)\n",
    "\n",
    "    # Empty string to store our results.\n",
    "    text_generated = []\n",
    "\n",
    "    # Here batch size == 1.\n",
    "    # model.reset_states()\n",
    "    for char_index in range(num_generate):\n",
    "        predictions = model(input_indices)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # Using a categorical distribution to predict the character returned by the model.\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(\n",
    "        predictions,\n",
    "        num_samples=1\n",
    "        )[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state.\n",
    "        input_indices = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(chars_from_ids(predicted_id).numpy().decode('UTF-8'))\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_16\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_16\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                    │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,938,304</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">119</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">121,975</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_16 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │        \u001b[38;5;34m30,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_15 (\u001b[38;5;33mGRU\u001b[0m)                    │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)        │     \u001b[38;5;34m3,938,304\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m119\u001b[0m)         │       \u001b[38;5;34m121,975\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,090,743</span> (15.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,090,743\u001b[0m (15.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,090,743</span> (15.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,090,743\u001b[0m (15.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simplified_batch_size = 1\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.build(tf.TensorShape([simplified_batch_size, None]))\n",
    "model.load_weights('./training_checkpoints/ckpt_40.weights.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A holdra úgy jutunk el, hogy milyen a súlyuk\n",
      "jóformán nagyobb volt, mint a természet erőinek munkáját - az ember munkáját sem sikerült megint betőlük. De a levegőben lebeg.\n",
      "Barbicane nyomára az első csákány vágásnál! - mondta az elnök.\n",
      "- Én meg az utolsó intézkedé-\n",
      "seket. Úgy rendezkedtek, mint két módszeres gondolkodású Barbicane, akinek más és más domborzatot\n",
      "alakítottak re. A másik oldalon a Hold verte vissza a nagy T mínusz s, gyalok kony?”-kró területen könnyen levetének előbb helyzetetbe való talpraesett beavatkozásának csúcsaihoz, hogy milyen a Hold vonzóereje egyensúlyban van; erről a pontról 50 000 másodperc, vagyis 83 óra és 20 perc alatt éri el azt a pontot, ahol a fák minden népe díszítettet. Csak egy\n",
      "történet rejtett ki kell azonban hamarosan megfogalmazott javaslatot. De mi lett türelmetlenül.\n",
      "- Mi akadálya van annak, hogy kielégítse a tökéletesség hirtelen megszűnésével olyan óriási hőség kell elérnek a hegyeknek a méretek alt a vonalak mivoltát. Ezek a\n",
      "volt ehhez a mostanáig\n",
      "elragadott. Mindenki m\n"
     ]
    }
   ],
   "source": [
    "# Generate the text with default temperature (1.0).\n",
    "print(generate_text(model, start_string=u\"A holdra úgy jutunk el, hogy\", temperature=0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
