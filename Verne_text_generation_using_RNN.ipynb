{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gitaroktato/deep-learning-exercises/blob/verne-rnn-generator/%20Verne_text_generation_using_RNN..ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smCVdpCXifWC"
   },
   "source": [
    "# Text generation in the style of Jules Verne using RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Setting up TensorFlow log level\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H766SuuViZ7q",
    "outputId": "e14d6b9c-fd8a-4496-9217-8f6dbdb4d0cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.20\n",
      "Tensorflow version: 2.17.0\n",
      "Keras version: 3.5.0\n",
      "NumPy version: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import platform\n",
    "import time\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "print('Python version:', platform.python_version())\n",
    "print('Tensorflow version:', tf.__version__)\n",
    "print('Keras version:', tf.keras.__version__)\n",
    "print('NumPy version:', np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cen2gAO0i2uA"
   },
   "source": [
    "## Loading the dataset\n",
    "Load the dataset and read the data. Take a look in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KGnb8Nk3i5xY",
    "outputId": "19162988-65dd-4d0b-c341-9cd538639835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 662630 characters\n"
     ]
    }
   ],
   "source": [
    "path_to_file = './docs/utazas_a_holdra.txt'\n",
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-K_dp5z6jYwx",
    "outputId": "7cc3da4a-1ba4-4c84-d7e6-8ba53087a57b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULES VERNE\n",
      "UTAZÁS A HOLDBA\n",
      "______\n",
      "UTAZÁS A HOLD KÖRÜL\n",
      "KÉT REGÉNY\n",
      "FORDÍTOTTA: KILÉNYI MÁRIA\n",
      "2\n",
      "TARTALOM\n",
      "UTAZÁS A HOLDBA\n",
      "1. A GUN CLUB\n",
      "2. BARBICANE ELNÖK BEJELENTÉSE\n",
      "3. A BEJELENTÉS HATÁSA\n",
      "4. A CAMBRIDGE-I CSILLAGVIZSGÁLÓ VÁLASZA\n",
      "5. A HOLD REGÉNYE\n",
      "6. A\n"
     ]
    }
   ],
   "source": [
    "# First few characters of the text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2O5JXa8_jjZ1",
    "outputId": "0781cecc-b77b-469f-c0f7-9399bb451fc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118 unique characters\n",
      "vocab: ['\\n', ' ', '!', '%', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '°', 'Á', 'É', 'Í', 'Ó', 'Ö', 'Ú', 'Ü', 'á', 'è', 'é', 'í', 'ó', 'ô', 'ö', 'ú', 'ü', 'Ő', 'ő', 'Ű', 'ű', 'π', '’', '”', '„', '−', '\\uf8eb', '\\uf8ec', '\\uf8ed', '\\uf8ee', '\\uf8ef', '\\uf8f0', '\\uf8f6', '\\uf8f7', '\\uf8f8', '\\uf8f9', '\\uf8fa', '\\uf8fb']\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file. Note, that it contains Hungarian character set.\n",
    "vocab = sorted(set(text))\n",
    "\n",
    "print('{} unique characters'.format(len(vocab)))\n",
    "print('vocab:', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F42fiPq2jt0_"
   },
   "source": [
    "# Process the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IKlLnwNJkLjJ",
    "outputId": "73f7e063-4214-49c0-804e-dcb53e7f067b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   1,\n",
      "  ' ' :   2,\n",
      "  '!' :   3,\n",
      "  '%' :   4,\n",
      "  \"'\" :   5,\n",
      "  '(' :   6,\n",
      "  ')' :   7,\n",
      "  '*' :   8,\n",
      "  '+' :   9,\n",
      "  ',' :  10,\n",
      "  '-' :  11,\n",
      "  '.' :  12,\n",
      "  '/' :  13,\n",
      "  '0' :  14,\n",
      "  '1' :  15,\n",
      "  '2' :  16,\n",
      "  '3' :  17,\n",
      "  '4' :  18,\n",
      "  '5' :  19,\n",
      "  '6' :  20,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)\n",
    "\n",
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "\n",
    "print('{')\n",
    "for char, _ in zip(vocab, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), ids_from_chars(char)))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XNLgvvcWs4RF",
    "outputId": "5bc9cbc4-4b27-4549-f5a7-3bffeb29c7b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_as_int length: 662630\n",
      "'JULES VERNE\\nUTA' --> array([37, 48, 39, 32, 46,  2, 49, 32, 45, 41, 32,  1, 48, 47, 28])\n"
     ]
    }
   ],
   "source": [
    "# Convert chars in text to indices.\n",
    "text_as_int = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "\n",
    "print('text_as_int length: {}'.format(len(text_as_int)))\n",
    "print('{} --> {}'.format(repr(text[:15]), repr(text_as_int[:15].numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQCgjcUhvtOE"
   },
   "source": [
    "## Create training examples and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KuopxsvYvwWa",
    "outputId": "67f147e0-5404-4a75-f202-3fd6d3820b02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples_per_epoch: 6560\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters.\n",
    "sequence_length = 100\n",
    "examples_per_epoch = len(text) // (sequence_length + 1)\n",
    "\n",
    "print('examples_per_epoch:', examples_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zmmgnTXnv2Cl",
    "outputId": "3ef6012e-2bb2-44dc-af76-37baca83e154"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J\n",
      "U\n",
      "L\n",
      "E\n",
      "S\n",
      " \n",
      "V\n",
      "E\n",
      "R\n",
      "N\n"
     ]
    }
   ],
   "source": [
    "# Create training dataset.\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for id in ids_dataset.take(10):\n",
    "    print(chars_from_ids(id).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gRTHHV3KwThJ",
    "outputId": "7964f66b-e81c-4673-b73d-d3f61434cf77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences count: 6560\n",
      "\n",
      "JULES VERNE\n",
      "UTAZÁS A HOLDBA\n",
      "______\n",
      "UTAZÁS A HOLD KÖRÜL\n",
      "KÉT REGÉNY\n",
      "FORDÍTOTTA: KILÉNYI MÁRIA\n",
      "2\n",
      "TARTALO\n",
      "M\n",
      "UTAZÁS A HOLDBA\n",
      "1. A GUN CLUB\n",
      "2. BARBICANE ELNÖK BEJELENTÉSE\n",
      "3. A BEJELENTÉS HATÁSA\n",
      "4. A CAMBRIDGE-\n",
      "I CSILLAGVIZSGÁLÓ VÁLASZA\n",
      "5. A HOLD REGÉNYE\n",
      "6. AMIT MINDEN AMERIKAINAK TUDNIA KELL, ÉS AMIT EGYETLEN \n",
      "AMERIKAINAK SEM\n",
      "SZABAD HINNIE TÖBBÉ\n",
      "7. AZ ÁGYÚGOLYÓ HIMNUSZA\n",
      "8. AZ ÁGYÚ TÖRTÉNETE\n",
      "9. A LŐPORKÉRDÉS\n",
      "10\n",
      ". HUSZONÖTMILLIÓ BARÁT ÉS EGYETLENEGY ELLENSÉG\n",
      "11. FLORIDA ÉS TEXAS\n",
      "12. URBI ET ORBI\n",
      "13. STONE’S HILL\n"
     ]
    }
   ],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
    "\n",
    "# Generate batched sequences out of the char_dataset.\n",
    "sequences = ids_dataset.batch(sequence_length + 1, drop_remainder=True)\n",
    "\n",
    "# Sequences size is the same as examples_per_epoch.\n",
    "print('Sequences count: {}'.format(len(list(sequences.as_numpy_iterator()))));\n",
    "print()\n",
    "\n",
    "# Sequences examples.\n",
    "for item in sequences.take(5):\n",
    "    print(text_from_ids(item).numpy().decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gSA60YaJz4Wu"
   },
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ChW0Y7i4z8Ob",
    "outputId": "9d7356c8-54bb-472e-edc6-1564d45655db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TVWfXsFe0A-_",
    "outputId": "173e3095-5d86-479c-ab5c-4c94cc8b7fef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 6560\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "# Dataset size is the same as examples_per_epoch.\n",
    "# But each element of a sequence is now has length of `sequence_length`\n",
    "# and not `sequence_length + 1`.\n",
    "print('dataset size: {}'.format(len(list(dataset.as_numpy_iterator()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24EgwAfo0L9R",
    "outputId": "3e63f541-c175-49e1-e86e-788b7b20c6e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence size: 100\n",
      "Target sequence size: 100\n",
      "\n",
      "Input: JULES VERNE\n",
      "UTAZÁS A HOLDBA\n",
      "______\n",
      "UTAZÁS A HOLD KÖRÜL\n",
      "KÉT REGÉNY\n",
      "FORDÍTOTTA: KILÉNYI MÁRIA\n",
      "2\n",
      "TARTAL\n",
      "Target: ULES VERNE\n",
      "UTAZÁS A HOLDBA\n",
      "______\n",
      "UTAZÁS A HOLD KÖRÜL\n",
      "KÉT REGÉNY\n",
      "FORDÍTOTTA: KILÉNYI MÁRIA\n",
      "2\n",
      "TARTALO\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print('Input sequence size:', repr(len(input_example.numpy())))\n",
    "    print('Target sequence size:', repr(len(target_example.numpy())))\n",
    "    print()\n",
    "    print('Input:', text_from_ids(input_example).numpy().decode('UTF-8'))\n",
    "    print('Target:', text_from_ids(target_example).numpy().decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIbRGsaM0Zs5",
    "outputId": "c08f7b65-b947-41f2-8b62-922424189beb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  0\n",
      "  input: 37 (J)\n",
      "  expected output: 48 (U)\n",
      "Step  1\n",
      "  input: 48 (U)\n",
      "  expected output: 39 (L)\n",
      "Step  2\n",
      "  input: 39 (L)\n",
      "  expected output: 32 (E)\n",
      "Step  3\n",
      "  input: 32 (E)\n",
      "  expected output: 46 (S)\n",
      "Step  4\n",
      "  input: 46 (S)\n",
      "  expected output: 2 ( )\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print('Step {:2d}'.format(i))\n",
    "    print('  input: {} ({:s})'.format(input_idx, chars_from_ids(input_idx).numpy().decode('UTF-8')))\n",
    "    print('  expected output: {} ({:s})'.format(target_idx, chars_from_ids(target_idx).numpy().decode('UTF-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0svwTVeQMSvO"
   },
   "source": [
    "## Create training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pzfN6jY6MW7J",
    "outputId": "61c019ad-af31-4b9c-8be3-7a1f765bdcd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CW3ibDNoMmK_",
    "outputId": "38daf05c-a1ec-4fa1-c5ad-27d66db04ff1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched dataset size: 102\n"
     ]
    }
   ],
   "source": [
    "print('Batched dataset size: {}'.format(len(list(dataset.as_numpy_iterator()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VdeCBH2UMpiV",
    "outputId": "f0a924df-d22d-4d2a-dad9-c22930734926"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st batch: input_text: tf.Tensor(\n",
      "[[ 2 67 59 ... 73 80 89]\n",
      " [68 91 68 ... 72 80 91]\n",
      " [12  1 28 ... 74 69 74]\n",
      " ...\n",
      " [57 55 68 ...  2 55 65]\n",
      " [67 69 68 ... 72 97 66]\n",
      " [66 65 59 ...  2 56 55]], shape=(64, 100), dtype=int64)\n",
      "\n",
      "1st batch: target_text: tf.Tensor(\n",
      "[[67 59 74 ... 80 89 67]\n",
      " [91 68 65 ... 80 91 73]\n",
      " [ 1 28  2 ... 69 74 74]\n",
      " ...\n",
      " [55 68 59 ... 55 65 65]\n",
      " [69 68 58 ... 97 66 68]\n",
      " [65 59 58 ... 56 55 72]], shape=(64, 100), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for input_text, target_text in dataset.take(1):\n",
    "    print('1st batch: input_text:', input_text)\n",
    "    print()\n",
    "    print('1st batch: target_text:', target_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_wmq4O2M_NV"
   },
   "source": [
    "## Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8rEHAg9LNB1c",
    "outputId": "a1ea7019-13b7-408a-f1a6-45bd825db65b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "tmp_input_array shape: (2, 8)\n",
      "tmp_input_array:\n",
      "[[9 0 6 7 2 8 3 6]\n",
      " [8 7 0 7 2 0 2 0]]\n",
      "\n",
      "tmp_output_array shape: (2, 8, 5)\n",
      "tmp_output_array:\n",
      "[[[-0.00966778  0.04374819  0.00912174  0.00637436 -0.00536316]\n",
      "  [ 0.03929282  0.02441926 -0.00540962 -0.03715355  0.028134  ]\n",
      "  [ 0.0287972  -0.04842454 -0.00875903  0.03096397 -0.00944003]\n",
      "  [-0.04878235  0.03456947 -0.0036199  -0.00601719  0.00349481]\n",
      "  [-0.02845841 -0.01485568  0.01543272 -0.00357848 -0.04306713]\n",
      "  [-0.01974423 -0.01239475 -0.00883179 -0.03303087 -0.02780288]\n",
      "  [ 0.02681014 -0.03628205  0.01062544 -0.0042259   0.03576546]\n",
      "  [ 0.0287972  -0.04842454 -0.00875903  0.03096397 -0.00944003]]\n",
      "\n",
      " [[-0.01974423 -0.01239475 -0.00883179 -0.03303087 -0.02780288]\n",
      "  [-0.04878235  0.03456947 -0.0036199  -0.00601719  0.00349481]\n",
      "  [ 0.03929282  0.02441926 -0.00540962 -0.03715355  0.028134  ]\n",
      "  [-0.04878235  0.03456947 -0.0036199  -0.00601719  0.00349481]\n",
      "  [-0.02845841 -0.01485568  0.01543272 -0.00357848 -0.04306713]\n",
      "  [ 0.03929282  0.02441926 -0.00540962 -0.03715355  0.028134  ]\n",
      "  [-0.02845841 -0.01485568  0.01543272 -0.00357848 -0.04306713]\n",
      "  [ 0.03929282  0.02441926 -0.00540962 -0.03715355  0.028134  ]]]\n"
     ]
    }
   ],
   "source": [
    "# Let's do a quick detour and see how Embeding layer works.\n",
    "# It takes several char indices sequences (batch) as an input.\n",
    "# It encodes every character of every sequence to a vector of tmp_embeding_size length.\n",
    "tmp_vocab_size = 10\n",
    "tmp_embeding_size = 5\n",
    "tmp_input_length = 8\n",
    "tmp_batch_size = 2\n",
    "\n",
    "tmp_model = tf.keras.models.Sequential()\n",
    "tmp_model.add(tf.keras.layers.Embedding(\n",
    "  input_dim=tmp_vocab_size,\n",
    "  output_dim=tmp_embeding_size,\n",
    "  input_length=tmp_input_length\n",
    "))\n",
    "# The model will take as input an integer matrix of size (batch, input_length).\n",
    "# The largest integer (i.e. word index) in the input should be no larger than 9 (tmp_vocab_size).\n",
    "# Now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "tmp_input_array = np.random.randint(\n",
    "  low=0,\n",
    "  high=tmp_vocab_size,\n",
    "  size=(tmp_batch_size, tmp_input_length)\n",
    ")\n",
    "tmp_model.compile('rmsprop', 'mse')\n",
    "tmp_output_array = tmp_model.predict(tmp_input_array)\n",
    "\n",
    "print('tmp_input_array shape:', tmp_input_array.shape)\n",
    "print('tmp_input_array:')\n",
    "print(tmp_input_array)\n",
    "print()\n",
    "print('tmp_output_array shape:', tmp_output_array.shape)\n",
    "print('tmp_output_array:')\n",
    "print(tmp_output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "4DjgSuB7NRkq"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars.\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension.\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units.\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "1YWPEdwvNV41"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Embedding(\n",
    "      input_dim=vocab_size,\n",
    "      output_dim=embedding_dim\n",
    "    ))\n",
    "\n",
    "    model.add(tf.keras.layers.GRU(\n",
    "      units=rnn_units,\n",
    "      return_sequences=True,\n",
    "      stateful=True,\n",
    "      recurrent_initializer=tf.keras.initializers.GlorotNormal()\n",
    "    ))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(vocab_size))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "umanu3FvNeU3"
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4hG7uBGO8f9"
   },
   "source": [
    "## Try the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S34880x-Odjb",
    "outputId": "5a9ed68e-bca2-4d65-9723-1dc7a04f73a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100)\n",
      "(64, 100, 119) # (batch_size, sequence_length, vocab_size)\n",
      "(64, 100)\n",
      "(64, 100, 119) # (batch_size, sequence_length, vocab_size)\n",
      "(64, 100)\n",
      "(64, 100, 119) # (batch_size, sequence_length, vocab_size)\n",
      "(64, 100)\n",
      "(64, 100, 119) # (batch_size, sequence_length, vocab_size)\n",
      "(64, 100)\n",
      "(64, 100, 119) # (batch_size, sequence_length, vocab_size)\n",
      "(64, 100)\n",
      "(64, 100, 119) # (batch_size, sequence_length, vocab_size)\n",
      "(64, 100)\n",
      "(64, 100, 119) # (batch_size, sequence_length, vocab_size)\n",
      "(64, 100)\n",
      "(64, 100, 119) # (batch_size, sequence_length, vocab_size)\n",
      "(64, 100)\n",
      "(64, 100, 119) # (batch_size, sequence_length, vocab_size)\n",
      "(64, 100)\n",
      "(64, 100, 119) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(10):\n",
    "    print(input_example_batch.shape)\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "tC_xiJEtOo7D",
    "outputId": "a6b7c38f-3a1d-49f1-bcf8-f9cc44fe4135"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,938,304</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">119</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">121,975</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │        \u001b[38;5;34m30,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m)        │     \u001b[38;5;34m3,938,304\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m119\u001b[0m)         │       \u001b[38;5;34m121,975\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,090,743</span> (15.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,090,743\u001b[0m (15.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,090,743</span> (15.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,090,743\u001b[0m (15.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wQMG94pYO_5U",
    "outputId": "f8d03b07-c5aa-46cb-e1e4-5fbe77740a2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the 1st letter of the batch 1st sequense:\n",
      "tf.Tensor(\n",
      "[ 3.26852710e-03 -1.28624793e-02 -2.20517572e-02  1.00101717e-02\n",
      "  2.70425482e-03  3.19789676e-03 -4.59093275e-03  1.02390293e-02\n",
      "  8.40921421e-03  2.50221156e-02  1.61201581e-02 -9.49472841e-03\n",
      "  1.00166844e-02 -4.02102387e-03  1.47341397e-02 -4.94778482e-03\n",
      " -1.39987674e-02 -1.97633971e-02  3.47917303e-02 -1.07196933e-02\n",
      "  7.66686583e-03  9.85303614e-03  4.75160219e-03 -6.50190655e-03\n",
      "  3.26241972e-03 -1.09491069e-02  1.42350020e-02 -1.32487398e-02\n",
      " -3.43950908e-03 -1.10880602e-02  7.24065490e-03  1.00938207e-03\n",
      " -2.16816249e-03 -6.67137513e-03  5.79622341e-03  7.57358794e-05\n",
      " -1.95990205e-02  1.62121821e-02  1.54080503e-02 -1.15883434e-02\n",
      " -2.49495753e-03 -4.37521795e-03 -8.93631193e-04 -3.36307324e-02\n",
      "  4.18191601e-04 -1.16866757e-03 -1.37997186e-02  8.57010856e-03\n",
      "  9.82117746e-03  5.32331550e-03 -2.37127896e-02 -5.70450118e-03\n",
      " -2.26037558e-02 -7.68663036e-03  6.27195975e-03 -6.88095903e-03\n",
      "  6.33839564e-03 -1.19171152e-02  1.41948462e-02 -1.54402647e-02\n",
      " -1.30508868e-02 -9.18132346e-03 -1.33526255e-03 -2.60206331e-02\n",
      " -1.39761679e-02  1.24501232e-02 -8.69082566e-03  9.62871965e-03\n",
      "  5.97534189e-03  4.41042753e-03  3.46164294e-02 -2.31478689e-03\n",
      "  3.11843678e-03 -1.41637502e-02  6.04583509e-03  2.02920213e-02\n",
      "  2.41191592e-03 -1.17345136e-02 -6.44598855e-03 -2.06099357e-03\n",
      " -1.70382913e-02  5.06108394e-03 -3.23693175e-03 -7.78539921e-04\n",
      "  1.57723145e-03  9.19520669e-03  2.26306077e-03  1.95199884e-02\n",
      "  2.59205606e-02  1.54622446e-03 -7.57261366e-03  1.03932023e-02\n",
      " -7.45198783e-03 -1.47719057e-02 -1.84400112e-03 -1.82340089e-02\n",
      "  6.38616597e-03  1.54133346e-02  2.29035993e-03  1.64999682e-02\n",
      " -6.96222577e-03 -2.08430421e-02 -1.78691950e-02  1.78850219e-02\n",
      " -7.83520099e-03  8.48225690e-03 -9.36887693e-03 -4.80269548e-03\n",
      " -1.27048055e-02 -3.13755753e-03  1.29187051e-02 -2.01917384e-02\n",
      "  2.19347235e-02 -2.70537008e-03  2.24847440e-02 -4.88016754e-03\n",
      "  1.32733129e-03  8.53937760e-04  2.13542650e-03], shape=(119,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('Prediction for the 1st letter of the batch 1st sequense:')\n",
    "print(example_batch_predictions[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9vtg8JMPNiw"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jm_E3PmzPOzh",
    "outputId": "e880f6e2-3e52-4545-8d15-53f9f8b613f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 119)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.779363\n"
     ]
    }
   ],
   "source": [
    "# An objective function.\n",
    "# The function is any callable with the signature scalar_loss = fn(y_true, y_pred).\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(\n",
    "      y_true=labels,\n",
    "      y_pred=logits,\n",
    "      from_logits=True\n",
    "    )\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "N9fTz4hAPS7O"
   },
   "outputs": [],
   "source": [
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    optimizer=adam_optimizer,\n",
    "    loss=loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3CgV5q0PbVw"
   },
   "source": [
    "## Configure checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Rp-r4sTzPd1N"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJOhfoErPi6k",
    "outputId": "b76b8d75-3934-4e21-acbb-9d2d87e33c4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 3.6104\n",
      "Epoch 2/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 2.3499\n",
      "Epoch 3/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 2.0544\n",
      "Epoch 4/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 1.8317\n",
      "Epoch 5/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 64ms/step - loss: 1.6695\n",
      "Epoch 6/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - loss: 1.5515\n",
      "Epoch 7/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 1.4740\n",
      "Epoch 8/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 1.4002\n",
      "Epoch 9/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 1.3436\n",
      "Epoch 10/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 1.2983\n",
      "Epoch 11/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 1.2460\n",
      "Epoch 12/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 1.1970\n",
      "Epoch 13/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 64ms/step - loss: 1.1568\n",
      "Epoch 14/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 1.1088\n",
      "Epoch 15/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 1.0654\n",
      "Epoch 16/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 1.0232\n",
      "Epoch 17/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step - loss: 0.9769\n",
      "Epoch 18/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.9312\n",
      "Epoch 19/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.8875\n",
      "Epoch 20/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.8448\n",
      "Epoch 21/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 64ms/step - loss: 0.8023\n",
      "Epoch 22/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.7618\n",
      "Epoch 23/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.7226\n",
      "Epoch 24/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.6874\n",
      "Epoch 25/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.6518\n",
      "Epoch 26/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.6208\n",
      "Epoch 27/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.5933\n",
      "Epoch 28/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.5677\n",
      "Epoch 29/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 64ms/step - loss: 0.5467\n",
      "Epoch 30/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.5229\n",
      "Epoch 31/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.5068\n",
      "Epoch 32/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.4930\n",
      "Epoch 33/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.4777\n",
      "Epoch 34/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.4680\n",
      "Epoch 35/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.4570\n",
      "Epoch 36/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.4473\n",
      "Epoch 37/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 64ms/step - loss: 0.4370\n",
      "Epoch 38/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.4320\n",
      "Epoch 39/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.4266\n",
      "Epoch 40/40\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.4213\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=40\n",
    "\n",
    "import keras\n",
    "\n",
    "keras.config.disable_traceback_filtering()\n",
    "# keras.backend.set_image_data_format(\"channels_last\")\n",
    "\n",
    "history = model.fit(\n",
    "  x=dataset,\n",
    "  epochs=EPOCHS,\n",
    "  callbacks=[\n",
    "    checkpoint_callback\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_generate\n",
    "# - number of characters to generate.\n",
    "#\n",
    "# temperature\n",
    "# - Low temperatures results in more predictable text.\n",
    "# - Higher temperatures results in more surprising text.\n",
    "# - Experiment to find the best setting.\n",
    "def generate_text(model, start_string, num_generate = 1000, temperature=1.0):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing).\n",
    "    input_indices = [ids_from_chars(s) for s in start_string]\n",
    "    input_indices = tf.expand_dims(input_indices, 0)\n",
    "\n",
    "    # Empty string to store our results.\n",
    "    text_generated = []\n",
    "\n",
    "    # Here batch size == 1.\n",
    "    # model.reset_states()\n",
    "    for char_index in range(num_generate):\n",
    "        predictions = model(input_indices)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # Using a categorical distribution to predict the character returned by the model.\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(\n",
    "        predictions,\n",
    "        num_samples=1\n",
    "        )[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state.\n",
    "        input_indices = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(chars_from_ids(predicted_id).numpy().decode('UTF-8'))\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,938,304</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">119</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">121,975</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │        \u001b[38;5;34m30,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)        │     \u001b[38;5;34m3,938,304\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m119\u001b[0m)         │       \u001b[38;5;34m121,975\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,090,743</span> (15.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,090,743\u001b[0m (15.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,090,743</span> (15.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,090,743\u001b[0m (15.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simplified_batch_size = 1\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.build(tf.TensorShape([simplified_batch_size, None]))\n",
    "model.load_weights('./training_checkpoints/ckpt_40.weights.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A holdra úgy jutunk el, hogy minden nehézséget állított elő - folytatta Barbicane -, miután a torkok számára. Ezek a térképek megközelítőerejének ellen a lövedék kilövésénél, de addig szó sem lehet ilyen tárgyak esése azonban s a tömegek kívánsága.\n",
      "S a tömeg egy\n",
      "egyszerre két tengert is megérteni az űrben ment végbe, ahol a levegő molekuláit semmi sem tartaná az állapotában. Ez a férfiaké. Latona és Jupiter e\n",
      "leárnyala izgalmas pillanatában keletkező lökést. Mert éppenséggel\n",
      "nem mindegy, hogy a lövedék már csak egy meteort a világűr magassága a\n",
      "megfigyeléseket.\n",
      "A megfigyelők szeme előtt ismét feltűnt a holdtányér egyik szélétől a másikig\n",
      "követhette volna. Barbicane elnök elgondolása kezdetben helyes magassága ezáltal is megérzik, mert a vonatok az olvasztott fémnek kellett kitölteni, amelyek a bolygók közé, a Herschel-földek itt kellett kikeresni egy pontot, ahol a két égitest vonzóereje\n",
      "kiegyenlítődik, a lövedék kilövésénél, de addig szó sem jutott társainkat, mint az északi sarkig terjed. A Hold szerep-\n",
      "hely fö\n"
     ]
    }
   ],
   "source": [
    "# Generate the text with default temperature (1.0).\n",
    "print(generate_text(model, start_string=u\"A holdra úgy jutunk el, hogy\", temperature=0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
