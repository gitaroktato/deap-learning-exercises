{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitaroktato/deep-learning-exercises/blob/text_classification_simple/text_classification_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euN_yFkMEbef"
      },
      "source": [
        "# Text classification from scratch\n",
        "\n",
        "**Authors:** Mark Omernick, Francois Chollet<br>\n",
        "**Date created:** 2019/11/06<br>\n",
        "**Last modified:** 2020/05/17<br>\n",
        "**Description:** Text sentiment classification starting from raw text files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZgvBhBREbeg"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This example shows how to do text classification starting from raw text (as\n",
        "a set of text files on disk). We demonstrate the workflow on the IMDB sentiment\n",
        "classification dataset (unprocessed version). We use the `TextVectorization` layer for\n",
        " word splitting & indexing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD976NnuEbeh"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lSLYaRMwEbeh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQvSvlctEbeh"
      },
      "source": [
        "## Load the data: IMDB movie review sentiment classification\n",
        "\n",
        "Let's download the data and inspect its structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qikGdtC4Ebeh",
        "outputId": "bd15b3b9-b59a-44bb-b2e7-effe3779cf63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  2814k      0  0:00:29  0:00:29 --:--:-- 3978k\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGHVpvvjEbei"
      },
      "source": [
        "The `aclImdb` folder contains a `train` and `test` subfolder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zRcEsUefEbej",
        "outputId": "86511dd8-a754-47b6-c4f8-fd61a8430f5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imdbEr.txt  imdb.vocab\tREADME\ttest  train\n"
          ]
        }
      ],
      "source": [
        "!ls aclImdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4Mt0lZETEbej",
        "outputId": "4d0a18e7-3cba-4435-f780-98fdb06215ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\n"
          ]
        }
      ],
      "source": [
        "!ls aclImdb/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BvpU3MI_Ebek",
        "outputId": "6270ec84-bac4-4bab-8dee-6868c888393d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labeledBow.feat  neg  pos  unsup  unsupBow.feat  urls_neg.txt  urls_pos.txt  urls_unsup.txt\n"
          ]
        }
      ],
      "source": [
        "!ls aclImdb/train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgiSPmQ8Ebel"
      },
      "source": [
        "The `aclImdb/train/pos` and `aclImdb/train/neg` folders contain text files, each of\n",
        " which represents one review (either positive or negative):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0RyBb-2nEbem",
        "outputId": "fc241641-4339-4b3b-c570-4671bf9a843b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Being an Austrian myself this has been a straight knock in my face. Fortunately I don't live nowhere near the place where this movie takes place but unfortunately it portrays everything that the rest of Austria hates about Viennese people (or people close to that region). And it is very easy to read that this is exactly the directors intention: to let your head sink into your hands and say \"Oh my god, how can THAT be possible!\". No, not with me, the (in my opinion) totally exaggerated uncensored swinger club scene is not necessary, I watch porn, sure, but in this context I was rather disgusted than put in the right context.<br /><br />This movie tells a story about how misled people who suffer from lack of education or bad company try to survive and live in a world of redundancy and boring horizons. A girl who is treated like a whore by her super-jealous boyfriend (and still keeps coming back), a female teacher who discovers her masochism by putting the life of her super-cruel \"lover\" on the line, an old couple who has an almost mathematical daily cycle (she is the \"official replacement\" of his ex wife), a couple that has just divorced and has the ex husband suffer under the acts of his former wife obviously having a relationship with her masseuse and finally a crazy hitchhiker who asks her drivers the most unusual questions and stretches their nerves by just being super-annoying.<br /><br />After having seen it you feel almost nothing. You're not even shocked, sad, depressed or feel like doing anything... Maybe that's why I gave it 7 points, it made me react in a way I never reacted before. If that's good or bad is up to you!"
          ]
        }
      ],
      "source": [
        "!cat aclImdb/train/pos/6248_7.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9dsX-pjEbem"
      },
      "source": [
        "We are only interested in the `pos` and `neg` subfolders, so let's delete the other subfolder that has text files in it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YOAFI9yTEbem"
      },
      "outputs": [],
      "source": [
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk3_mFefEbem"
      },
      "source": [
        "You can use the utility `keras.utils.text_dataset_from_directory` to\n",
        "generate a labeled `tf.data.Dataset` object from a set of text files on disk filed\n",
        " into class-specific folders.\n",
        "\n",
        "Let's use it to generate the training, validation, and test datasets. The validation\n",
        "and training datasets are generated from two subsets of the `train` directory, with 20%\n",
        "of samples going to the validation dataset and 80% going to the training dataset.\n",
        "\n",
        "Having a validation dataset in addition to the test dataset is useful for tuning\n",
        "hyperparameters, such as the model architecture, for which the test dataset should not\n",
        "be used.\n",
        "\n",
        "Before putting the model out into the real world however, it should be retrained using all\n",
        "available training data (without creating a validation dataset), so its performance is maximized.\n",
        "\n",
        "When using the `validation_split` & `subset` arguments, make sure to either specify a\n",
        "random seed, or to pass `shuffle=False`, so that the validation & training splits you\n",
        "get have no overlap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xssXV7r7Ebem",
        "outputId": "ec448eed-2bf2-45f4-b92d-41e6be9bb7eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Number of batches in raw_train_ds: 625\n",
            "Number of batches in raw_val_ds: 157\n",
            "Number of batches in raw_test_ds: 782\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "raw_train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "\n",
        "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
        "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
        "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at the shape of our dataset\n",
        "# print(repr(raw_train_ds))\n",
        "# Getting the first element\n",
        "texts, labels = next(iter(raw_train_ds.take(1)))\n",
        "print('Texts shape: ', texts.shape)\n",
        "print('Texts sample: ', texts[:10].numpy())\n",
        "# Labels\n",
        "print('Texts shape: ', labels.shape)\n",
        "print('Texts sample: ', labels[:10].numpy())\n",
        "\n"
      ],
      "metadata": {
        "id": "fmhNsbbiFY69",
        "outputId": "e863c47a-0cf5-4f6c-ef81-4199308ddb5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texts shape:  (32,)\n",
            "Texts sample:  [b'This is one of the worst movies i have seen to date, the best part was Christian J. Meoli \"Leonard\" attempting to act jumping up and down outside the bar, kind-of like i wanted to do on the DVD, to spare the rest of humanity the agony of watching this shitty film. It has a great cast so you keep watching waiting for it to get good, i mean with Sean Astin \"Andrew\" (played his part perfectly, did a great job, too bad it was in this film), Kyra Sedgwick \"Bevan\", Ron Livingston \"Chad\", Ren\\xc3\\xa9e Zellweger \"Poet\" (they put her name on the cover she has a total of 1 line and less then 4 seconds in the whole movie...<br /><br />If the cast had any dignity, they would go out and buy all the copies of this film and burn them along with Writer / Director George Hickenlooper and Writer John Enbom'\n",
            " b\"I agree that Capital City should be on DVD. I watched this show only by accident in 1994 and fell in love with Rolf Saxon as Hudson Talbot. It was nice to see Americans who work abroad in London in the financial industry for a change. I loved Rolf in this role and loved every other role that he has been in. I can't believe the show only lasted 13 episodes. I liked William Armstrong as Hudson's flamboyant charming friend in the series. When they aired this show in the New York City area, it was always late at night or at off times. The show is less than an hour long. I felt this show should have gone on longer but the casting changes in the second season really made the show a little less interesting. I didn't care for Sylvia but missed the actress, Julia Phillips-Lane in the previous season. I felt this show took chances and often it worked. It showed Americans who loved and chose to live in London. The American characters were not arrogant or tried to outdo their British counterparts. I also liked the fact that they had tried to internationalize the cast rather than make them all British. I liked watching Julia Ormond in an early role. I felt this show should have lasted longer. I felt at times that the previews lasted as long as the show in less than an hour. They could have transferred the cast to New York City and it would have been a hit in America.\"\n",
            " b\"It was nice to see all the familiar characters again, but the story bothered me. We loved Ariel in the first movie- so why is the second one centered around her daughter? The new characters were annoying and I didn't like the plot of this. Worst of all, Christopher Daniel Barnes didn't come back as the voice of Eric! Disney, please stop remaking classic movies with these shoddy imitations.\"\n",
            " b'I haven\\'t actually finished the film. You may say that in this case I have no right to review it, especially so negatively. But I do, only because I stopped it on account of I couldn\\'t watch anymore...I got over halfway, and I only got there by promising myself something good was just around the corner. This film is so tiresome, so lackluster that I was actually insulted. I haven\\'t read many of the other reviews, so I\\'m not sure if there are other homosexual teens who have suffered through it, but I am homosexual, and I did go through \"similar\" revelations, day dreams, issues etc etc. There were maybe two moments where I actually felt this film could go somewhere, where I felt it may have some inkling of meaning, or relativity, but these hopes were dashed the moment the next set of clich\\xc3\\xa9-ridden narration came on. I mean, just look at the quotes on the IMDb page. Unfortunately you\\'re not able to hear the scratchy play back, nor the echo-ey fades if you\\'re just read the quotes, because they are just too painful/ridiculous/stupid to miss. I did give the film three stars, and all three of those stars go to the films cinematographer who did a fantastic job attempting to transform Archer\\'s tired \"concepts\" into something watchable. Mind you, I pray he wasn\\'t the one who decided to include all the long shots of TV closeups...another unnecessary clich\\xc3\\xa9 already over done in films such as Korine\\'s Gummo... I think it is extremely fitting that this film premiered at Sundance (only because Archer had connections in the festival via volunteer work he did, by the way...) because Sundance seems to be the one festival where clich\\xc3\\xa9 heavy drivel like this is still accepted as \"arthouse\". No, it\\'s not art house, I\\'m afraid it\\'s just plain s**t-house. Do not watch.'\n",
            " b'An excellent movie. Superb acting by Mary Alice, Phillip M. Thomas, and a young Irene Cara. Tony King was very realistic in his role of Satin. This movie was one of the last predominately \"all black\" movies of the 70\\'s and unlike the \"blaxploitation\" movies of that era, this movie actually had a plot, and was very well done. The movie soundtrack, sung by Aretha Franklin, was popular on the R&B charts at the time.'\n",
            " b\"We expected something great when we went to see this bomb. It is basically a Broadway play put on film. The music is plain terrible. There isn't one memorable song in the movie -- heard any hits from this movie? You won't because there aren't any. Some of the musical numbers go on so long that I got up to go to the restroom and get some pop corn and it was still going when I got back! If they were good songs well -- but they suck. The pace is slow, terrible character development. The lead was praised for her singing but sounded like she screamed every song -- it was almost impossible to stand. This movie has NOTHING to offer anyone but die-hard Broadway enthusiasts. This is without a doubt the most over rated movie I've seen in my entire life. A complete waist of time and money. There is nothing memorable about this movie except Danny Glover -- who wasn't on screen enough and whose character wasn't developed enough. Rent the video and you'll agree -- this movie was an expensive, over produced, polished dog do.\"\n",
            " b\"Serum starts as Eddie (Derek Phillips) is delighted to learn he has been accepted into medical school to carry on the family tradition of becoming an MD like his father Richard (Dennis O'Neill) & his uncle Eddie (David H. Hickey), however his joy could be short lived as Eddie is involved in an accident & is run over by a car. Taken to the nearest hospital it doesn't look good for poor Eddie so his uncle Eddie convinces his brother Richard to let him save Eddie with the serum he has developed, a serum which will give the recipient the power to self heal any sort of wound or illness. Desperate for his boy to live Richard agrees but the procedure has unwanted side effects like turning Eddie into a brain eating zombie which is just not a good thing...<br /><br />Executive produced, written & directed by Steve Franke I'll be perfectly frank myself & say Serum is awful, Serum is one of those no budget horror films which tries to rip-off other any number of other's & ends up being slightly more fun than having you fingernails pulled out with pliers. The script is terrible, it has the whole Re-Animator (1985) feel to it with mad scientists wielding huge syringes trying to eradicate death but it's so boring it's untrue, the first forty minutes is nothing more than a really dull soap opera that amounts to nothing expect to pad the running time out with Eddie arriving home after spending some time away & finding his ex-girlfriend has hooked up with someone else, arguments with his step-mom, getting drunk with his mate & generally boring the audience stiff. So, once the tedium of the first forty minutes is over & if your still watching it it takes another twenty minutes to get Eddie re-animated & then he kills a couple of people, police catch up with him & shoot him, the end. Thank god. Serum is devoid of any of the characteristic's that one would associate with a good film, the character's suck, the dialogue is poor, it takes itself far too seriously, it's dull, it's slow, it's forgettable & considering it's meant to be a horror film there's an alarming lack of blood, gore or horror. Not recommended, did I mention Serum was boring? I thought so.<br /><br />Director Franke does nothing to liven this thing up, although competent there's no style here at all. The gore levels are none existent, there's a bit of splashed blood, a bitten neck, a couple of scars on a dead woman's face, a couple of scenes where a needle pierces skin & that's it. Don't expect a Re-Animator in the gore department because if you do your going to be sorely disappointed, much like I was in fact. Filmed in what looks like one house, one restaurant & a lab the film has no variety either & just looks cheap throughout. There's a couple of scenes of nudity but that's nowhere near enough to save it.<br /><br />Technically the film isn't too bad, at least it looks like proper cameras were used, I can't really comment on the special effects because there aren't any but generally speaking Serum looks reasonably professional. Apparently shot in Texas, or should that read it should have literally been shot in Texas? The acting sucks although again I think they were proper actor's rather than friends or family of the director.<br /><br />Serum is a terrible film, it's dull, slow, boring, has no gore & feels like a horrible soap opera for the first forty minutes. I don't understand why anyone would feel the need to watch this when they can watch Re-Animator or one of it's sequels again instead, seriously I recommend you give Serum a miss. There I've just saved you from wasting 90 minutes of your life, you can thank me later.\"\n",
            " b'I couldn\\'t give this film a bad rating or bad review for two reasons: Robin Williams and Toni Collete. The film has the potential of being a thriller and there are some slight disturbing elements that lean to the psychological which was something the film could have focused a little on. Robin Williams plays Gabriel Noon, a storytelling night time deejay who is going through personal issues: his lover moves out and Gabriel is having what seems to be a case of storyteller\\'s block. One day he receives and reads a story written by a dying 14-year old boy named Pete Boland (Rory Culkin). Pete tells the story of his life and the abuse he suffered at the hands of his parents. He lives with his adopted mother and social worker, Donna Boland (Toni Collette). Gabriel is fascinated and begins a friendship with Pete, but things seem strange when Gabriel attempts to meet him and discovers the possibility that Pete Boland may not even exist. I won\\'t go into detail because I don\\'t want to spoil the film, but I will tell you this: it is quite predictable. Fascinating atmosphere for telling a story and good performances from Robin Williams and Toni Collette, who I thought was the film\\'s key character. Collette is without question one of the most talented and loveliest actresses. Her ability to tap into the psyche and personality of the characters she portrays is very uncanny and I hope to see her win an Oscar (hell, I think she might pull off getting a Best Supporting Actress nod for this one if the script were a little better). The film starts off as a psychological thriller, but a predictable one at that. If your curious to know the film\\'s ending and twists, then see the film otherwise I would rent another predictable thriller called \"Hide and Seek\".'\n",
            " b'For starters and for the record, the term \"Necromancy\" describes the black magic art of bringing the dead back to life and it does NOT, in any way, relate to having sex with cadavers. That is called necrophilia and, yes, I know it\\'s an obvious difference but I\\'m already getting a lot of remarks from acquaintances and relatives that I sport a perverted taste in movies! This movie is quite the opposite of perverted or sleazy, in fact, and merely just qualifies as boring, inept and terribly bad. \"Necromancy\" makes at least one top five ranking, namely in the list of most incoherent movies ever made! Now, director Bert I. Gordon is not exactly famous for delivering masterpieces (on his repertoire there are titles like \"Earth vs. the Spider\", \"King Dinosaur\" and \"Food of the Gods\") but he really surpassed himself here with a totally senseless, redundant and utterly nonsensical tale about witchcraft and secretive little towns. Shortly after the tragic experience of seeing their baby being born dead, Lori and her husband Frank move to the quiet little town of Lillith, where Frank suddenly got offered a prominent job in a toy factory. Lori is suspicious and senses an atmosphere of morbidity, especially with the town\\'s patriarch and \"owner\" Mr. Cato behaving very obtrusive and mysterious. That\\'s another thing. How can anybody \"own\" a town and everybody in it? Either way, Lori gradually discovers that everybody in Lillith is a witch and Mr. Cato exclusively lured her to the town because of her supernatural ability to resurrect the dead. Since many years already, Cato has been trying to bring his deceased son back to life and he\\'s prepared to make any human sacrifice it takes. I honestly don\\'t see the point of the whole movie. It\\'s a blatant rip-off of \"Rosemary\\'s Baby\" \\xc2\\x96 one of the alternate titles even is \"Rosemary\\'s Disciples\" \\xc2\\x96 but the script is muddled and imbecilic beyond belief. Why isn\\'t anyone allowed to have children for as long as Cato\\'s son remains dead? That\\'s just really selfish! When, where and how did Lori suddenly learn to resurrect the dead? \"Necromancy\" definitely contains a few genuinely uncanny and atmospheric moments, but these are unwarily accomplished either by complete coincidence or through a total lack of budget. The grainy photography provides the film with an eerie ambiance and the set pieces look cheap enough to be creepy. Orson Welles\\' performance \\xc2\\x96 undoubtedly the low point of his career \\xc2\\x96 is pitiable, and still it\\'s the best aspect about the entire movie.'\n",
            " b'I was expecting this to be the same kind of schlock as the previous Modesty Blaise movie, which is why I left it unwatched for so long, but I was very pleasantly surprised.<br /><br />Far from being a succession of silly gun battles and car/boat chases, it was an almost thoughtful analysis of how a pretty girl gets to become as hard as nails, with nothing being overstated or over-rationalized.<br /><br />It\\'s likely that the budgetary constraints actually helped with that: less time and effort was spent on finding ever-stupider ways for stunt men to pretend to die, and more was dedicated to making the movie worth watching. Hell, the biggest gun battle takes place off screen -- and the scene where it is heard is all the better for that background noise, that adds to the suspense -- who\\'s winning? Who\\'s dying?<br /><br />Alexandra Staden might not be as drop-dead gorgeous as Monica Vitti, but few are, and she certainly has every ounce of class and fire that\\'s needed to make the character work -- and the shape of her face, her hair, and her tall, slender body could have been lifted straight from the comic-strip graphics.<br /><br />Nikolaj Coaster-Waldau was the perfect choice for a Blaise bad-guy, in that he made the character interesting and enjoyable to watch -- even likable (and I doubt I\\'d consider taking on many brutal, psychopathic murderers as drinking buddies). I can\\'t think of a single one of Hollywood\\'s \"former waiters\" who could have pulled the role off that well.<br /><br />Fortunately, Blaise baddies always die, in the end (no spoilers there!) That\\'s a really good thing, because all the girls who would have spent their time swooning over such a disgustingly handsome and interesting hunk can now pragmatically settle for us ordinary Joes.']\n",
            "Texts shape:  (32,)\n",
            "Texts sample:  [0 1 0 0 1 0 0 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIrZMRBfEben"
      },
      "source": [
        "Let's preview a few samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "G1k1gsH6Eben",
        "outputId": "048745e9-02fe-4dcf-cc74-e8f784596c2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'Watching this was like getting a large mackerel slapped in your face over and over again. Even when you thought, \"That mackerel surely can\\'t be coming around again,\" *slap* there it was. I\\'m not sure what they were thinking. This is the sort of pilot I watched and wondered, \"Did the actors know they were on a doomed ship destined to never be made into a series?\" Not only black stereotypes but Swedish and Indian ones as well. And while \"Blazing Saddles\" made these stereotypes into a mix of comedy and uncomfortableness, these stereotypes were just downright offensive. There was no plot line, the ending was slapped on, and the jokes aren\\'t. Still, if you are a student of comedy, watch this pilot to see what you shouldn\\'t do.'\n",
            "0\n",
            "b\"Imagine pulling back the mask of a lethal assassin and finding Barbara Cartland there... that's what happens with this film.<br /><br />The opening showed promise, but soon it drops all pretenses of being a thriller (or even an imaginative love story) and the only reason they made this story becomes abundantly clear: to fill a gap in their female viewing market by creating yet another re-hash of 'mis-understood, brooding bad-boy' (Andrei) meets 'innocent, whimsical beauty' (Paula). <br /><br />Rather than waste any time in creating an original premise, the filmmakers went straight for the money-shot: the bad boy being tamed by said whimsical beauty. Thence follows a string of insincere and heavily-clich\\xc3\\xa9d love scenes sprinkled with pseudo philosophical/poetic fluff. Andrei's admission of being (eponymously) a 'poet' is levered in to round out the perceived qualities a Byronic hero should have - but even when we're told in heavy, underlined writing who and what he is, it's still difficult to believe it - or care.<br /><br />For a Byronic hero/antihero to work, the story needs subtlety, style and innovation - all of which are utterly absent here. This is not a modern day Phantom of the Opera, it's just what happens when a weak and rather silly woman (with loose knicker elastic) dates a bad man, who, after meeting her, seems as dangerous as bunny slippers.<br /><br />The performances might have saved this film, had they been any good: the female lead is preoccupied with looking sexy and 'otherworldly', no matter how forced or ridiculous; and poor Dougray Scott appears to have been drugged as he shambles through his part. This is not his best work. The glimmers of interest were brought by J\\xc3\\xbcrgen Prochnow as 'Vashon', and Andrew Lee Potts as the young photographer/brother. A better movie would have offed the sister and kept the brother instead.\"\n",
            "0\n",
            "b'I admit the problem I have with the much-celebrated Ealing films I\\'ve seen so far could be mine. To my taste, either they are black and rippingly funny, or so light in tone to be unsatisfying as comedies or stories. That\\'s a self-important way of saying I wanted to like \"The Man In The White Suit\" but found myself struggling to sit through its short run time.<br /><br />Textile worker Sidney Stratton (Alec Guinness) may be meek in manner, but he is doggedly committed to progress in the form of his attempts to invent a strand of fabric that can\\'t be broken or made dirty. Using a factory lab for his latest experiment, he toils against limitations both material and human - the latter being the benighted mill bosses who don\\'t understand what he is up to, then figure it out and become even more committed to stopping him.<br /><br />\"It\\'s small minds like yours that stand in the way of progress,\" Sidney complains, practicing in the mirror what he struggles to say to the Man.<br /><br />One problem with \"Man In The White Suit\" is that Sidney\\'s vision of progress is awfully small-minded, too, more so even than that of the bosses or the laborers who also resent his work. My problem is more elemental: For a comedy, \"White Suit\" is not funny. It\\'s a rather earnest script which too often tries to mine its feeble attempts at humor from spit-takes, double-takes, triple-takes, and dizzy takes.<br /><br />The best joke is the sound of the machine Sidney toils at, going \"Bleep-Blop-Bleep-Bop\" endlessly and fetching queer looks from every visitor until Sidney either extracts his miracle from it or blows it up trying. Like every other bit of stray humor that functions decently in this film, it\\'s leaned on too long.<br /><br />I\\'ve never seen Guinness less affecting in a movie, even though he looks impossibly young and earnest (though actually in his mid-30s). He seems so bloodless, even more so than the wax-faced general he played in \"Doctor Zhivago\" He\\'s the same cold fish whether he\\'s ignoring the sad affections of the affecting mill girl who offers to give him her life savings when he loses his job (pan-faced Vida Hope as Bertha) or the more sultry charms of young Daphne Birnley (Joan Greenwood), his one real ally in his fight against \"shabbiness and dirt\", as she puts it, making those words sound as impossibly sexy as only Greenwood could.<br /><br />Supporting players make \"White Suit\" work as well as it does. Ernest Thesinger of \"Bride Of Frankenstein\" fame plays a singularly nasty captain of industry who looks like Nosferatu and makes a laugh like a death rattle. Howard Marion-Crawford as another factory leader is as memorable here as he was playing a blinkered medical officer in \"Lawrence Of Arabia\". Then there\\'s the undeniable charm of Mandy Miller as a little girl who steals her few moments on camera right from under everyone else.<br /><br />But most of the scenes are played so straight that one wouldn\\'t think director Alexander Mackendrick had ever worked on a comedy before (his previous Ealing comedy \"Whisky Galore\" doesn\\'t reverse that impression, alas). Roger MacDougall\\'s play posits the notion of scientific progress as potential disaster, but fails to present dull Sidney in anything other than the most blandly pleasant of lights.<br /><br />Ealing comedies are remembered for capturing the human side of comedy. Yet the Ealings I\\'ve seen never seem to do this, working only when they play aggressively against our own sympathies. \"Kind Hearts And Coronets\" and \"The Ladykillers\" (Mackendrick again, go figure) are classics this way. \"White Suit\", on the other hand, is a pointless ramble that falls apart when it should cohere, just like that unfortunate suit.'\n",
            "0\n",
            "b'Went to watch this movie expecting a \\'nothing really much\\' action flick, still got very disappointed. The opening scene promised a little action with a tinge of comedy. It keeps you hooked for the first half coz till then you are expecting that now its time for the action to kick in. Well, nothing of that sort happens. The movie drags and the ending just thumps you down to a point that you get annoyed.Wonder what was the director thinking. Made no sense watsoever. The movie lacked in all aspects, had no real storyline and it seemed very hollow, even if \"Rambo\" was in it, I don\\'t think he could have helped the rating at all. There is simply no logic to the movie. A perfect way to waste your time and money. By far the most irritating movie i have ever seen and i am sure there will b others who\\'ll have the same viewpoint after enduring it. Definitely not for people who have a little movie sense left in them.'\n",
            "0\n",
            "b'I remember hitch hiking to Spain at 25, getting a lift from, what turned out to be, two fleeing Italian small crooks. They were doing a lot outside the law, but from the other side carrying a little portrait of Jesus in the pocket for their protection...Just and unjust, good and bad, criminal and correct where here in a new combination, outside of the categories I used to know. \\'Les Valseuses\\' gives me, although a film and not real life, a picture close to my own experiences: the intenseness of each moment as soon as you leave \\'all behind\\' and go for the momentous, whatever comes your way, it\\'s another state of mind and also \\'dangerous\\' form of life, because, as we all know, there are people who are not ready for this and willing to persecute you for \\'stealing\\' and so on...This film touches \\'values\\', it\\'s a story about \\'what\\'s right and wrong\\': morals. It\\'s resurrection of the individual fighting him/ herself free against the \\'false morals\\' and conformism...There\\'s danger all the way, because, how far can you go with your own \\'freedom\\' and crossing your own moral borders and that of other people? What to do with people who are willing to hurt you, put you in jail or even shoot at you for the things that you do, like \"stealing\" some petrol from a multinational oil company for you fifth hand car? Les Valseuses re-awakens these questions in me, because morality, in contradiction to the usual \\'media message\\', is quite complex...'\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "# It's important to take a look at your raw data to ensure your normalization\n",
        "# and tokenization will work as expected. We can do that by taking a few\n",
        "# examples from the training set and looking at them.\n",
        "# This is one of the places where eager execution shines:\n",
        "# we can just evaluate these tensors using .numpy()\n",
        "# instead of needing to evaluate them in a Session/Graph context.\n",
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "    for i in range(5):\n",
        "        print(text_batch.numpy()[i])\n",
        "        print(label_batch.numpy()[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiKWni7PEben"
      },
      "source": [
        "## Prepare the data\n",
        "\n",
        "In particular, we remove `<br />` tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "8fnmQvN3Ebeo"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "\n",
        "# Having looked at our data above, we see that the raw text contains HTML break\n",
        "# tags of the form '<br />'. These tags will not be removed by the default\n",
        "# standardizer (which doesn't strip HTML). Because of this, we will need to\n",
        "# create a custom standardization function.\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Model constants.\n",
        "max_features = 20000\n",
        "embedding_dim = 128\n",
        "sequence_length = 500\n",
        "\n",
        "# Now that we have our custom standardization, we can instantiate our text\n",
        "# vectorization layer. We are using this layer to normalize, split, and map\n",
        "# strings to integers, so we set our 'output_mode' to 'int'.\n",
        "# Note that we're using the default split function,\n",
        "# and the custom standardization defined above.\n",
        "# We also set an explicit maximum sequence length, since the CNNs later in our\n",
        "# model won't support ragged sequences.\n",
        "vectorize_layer = keras.layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "# Now that the vectorize_layer has been created, call `adapt` on a text-only\n",
        "# dataset to create the vocabulary. You don't have to batch, but for very large\n",
        "# datasets this means you're not keeping spare copies of the dataset in memory.\n",
        "\n",
        "# Let's make a text-only dataset (no labels):\n",
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "# Let's call `adapt`:\n",
        "vectorize_layer.adapt(text_ds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seeing how `vectorize_layer` works\n",
        "vectorize_layer_smaller = keras.layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=12,\n",
        ")\n",
        "vectorize_layer_smaller.adapt(text_ds)\n",
        "# Comparing two sentences\n",
        "vector_first = vectorize_layer_smaller(\"Hello world! This is a sample text.\")\n",
        "vector_second = vectorize_layer_smaller(\"Hi everyone! Providing some example chars\")\n",
        "print('First vector: ', repr(vector_first))\n",
        "print('Second vector: ', repr(vector_second))\n",
        "print('Diff: ', vector_second - vector_first)"
      ],
      "metadata": {
        "id": "hXRQOMD4MiLc",
        "outputId": "a0e8a210-9838-4f13-e24b-96fc37292900",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First vector:  <tf.Tensor: shape=(12,), dtype=int64, numpy=\n",
            "array([ 4833,   185,    10,     7,     4, 10944,  3250,     0,     0,\n",
            "           0,     0,     0])>\n",
            "Second vector:  <tf.Tensor: shape=(12,), dtype=int64, numpy=\n",
            "array([9586,  309, 3465,   46,  446,    1,    0,    0,    0,    0,    0,\n",
            "          0])>\n",
            "Diff:  tf.Tensor(\n",
            "[  4753    124   3455     39    442 -10943  -3250      0      0      0\n",
            "      0      0], shape=(12,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xnpQ9izPNHYK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RrQh65eEbeo"
      },
      "source": [
        "## Two options to vectorize the data\n",
        "\n",
        "There are 2 ways we can use our text vectorization layer:\n",
        "\n",
        "**Option 1: Make it part of the model**, so as to obtain a model that processes raw\n",
        " strings, like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FfHsaDcEbeo"
      },
      "source": [
        "```python\n",
        "text_input = keras.Input(shape=(1,), dtype=tf.string, name='text')\n",
        "x = vectorize_layer(text_input)\n",
        "x = layers.Embedding(max_features + 1, embedding_dim)(x)\n",
        "...\n",
        "```\n",
        "\n",
        "**Option 2: Apply it to the text dataset** to obtain a dataset of word indices, then\n",
        " feed it into a model that expects integer sequences as inputs.\n",
        "\n",
        "An important difference between the two is that option 2 enables you to do\n",
        "**asynchronous CPU processing and buffering** of your data when training on GPU.\n",
        "So if you're training the model on GPU, you probably want to go with this option to get\n",
        " the best performance. This is what we will do below.\n",
        "\n",
        "If we were to export our model to production, we'd ship a model that accepts raw\n",
        "strings as input, like in the code snippet for option 1 above. This can be done after\n",
        " training. We do this in the last section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6Wv9OE1Ebeo"
      },
      "outputs": [],
      "source": [
        "\n",
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label\n",
        "\n",
        "\n",
        "# Vectorize the data.\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)\n",
        "\n",
        "# Do async prefetching / buffering of the data for best performance on GPU.\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qO71bBREbeo"
      },
      "source": [
        "## Build a model\n",
        "\n",
        "We choose a simple 1D convnet starting with an `Embedding` layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iV1CSiYyEbep"
      },
      "outputs": [],
      "source": [
        "# A integer input for vocab indices.\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "# 'embedding_dim'.\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Conv1D + global max pooling\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "\n",
        "model = keras.Model(inputs, predictions)\n",
        "\n",
        "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CYevYorEbep"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvICcNgBEbep"
      },
      "outputs": [],
      "source": [
        "epochs = 3\n",
        "\n",
        "# Fit the model using the train and test datasets.\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRBYGcv7Ebep"
      },
      "source": [
        "## Evaluate the model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rb437k2WEbep"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JzRrrDBEbeq"
      },
      "source": [
        "## Make an end-to-end model\n",
        "\n",
        "If you want to obtain a model capable of processing raw strings, you can simply\n",
        "create a new model (using the weights we just trained):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cogtzEhSEbeq"
      },
      "outputs": [],
      "source": [
        "# A string input\n",
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "# Turn strings into vocab indices\n",
        "indices = vectorize_layer(inputs)\n",
        "# Turn vocab indices into predictions\n",
        "outputs = model(indices)\n",
        "\n",
        "# Our end to end model\n",
        "end_to_end_model = keras.Model(inputs, outputs)\n",
        "end_to_end_model.compile(\n",
        "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "end_to_end_model.evaluate(raw_test_ds)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_classification_from_scratch",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}